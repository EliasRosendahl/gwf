#!/Users/mailund/anaconda/bin/python

import os
import os.path
import sys
import argparse
import pkg_resources
import gwf_workflow
from gwf_workflow.backends import AVAILABLE_BACKENDS
from gwf_workflow.configurations import read_configurations
from gwf_workflow.colours import *

# This gets the installed version of gwf that is actually being used by the script.
package_version = pkg_resources.require("gwf")[0].version
config = read_configurations()

parser = argparse.ArgumentParser(
    version=package_version,
    description="""
    Grid WorkFlow (v{version}) -- Keeps track of the status of jobs in a workflow and submits jobs that needs to be run.
    """.format(version=package_version),
    epilog="""
    For questions, comments or bug-reports contact <mailund@birc.au.dk> or go to https://github.com/mailund/gwf/issues/
    """)

parser.add_argument('-f', '--file', 
                    default='workflow.py', dest='workflow_file',
                    help='Workflow file if not the default (workflow.py).')
parser.add_argument('-d', '--dry-run', default=False, action='store_true',
                    help='The submit script will be printed but not executed.')
parser.add_argument('-s', '--status', default=False, action='store_true',
                    help='Print status of targets.')
parser.add_argument('-c', '--clean', default=False, action='store_true',
                    help='Delete output files generated by a target.')
parser.add_argument('--verbose', default=False, action='store_true', help='Verbose output.')


default_backend = config.get('gwf', 'backend')
parser.add_argument('--backend', default=default_backend,
                    choices=AVAILABLE_BACKENDS.keys(),
                    help='''Grid backend to use. If not specified the default ({default}) will be used.
                    '''.format(default=default_backend))


parser.add_argument('targets', nargs='*',
                    help='The targets to process. The default is all targets that are not completed or already running')

args = parser.parse_args()
if not os.path.exists(args.workflow_file):
    parser.error("The specified workflow file '{}' does not exist.\n".format(args.workflow_file))


# Set the backend based on defaults or arguments...
gwf_workflow.BACKEND = AVAILABLE_BACKENDS[args.backend]()


execfile(args.workflow_file)
from gwf_workflow.workflow import build_workflow, schedule, dependencies
workflow = build_workflow()

if args.clean:
    if len(args.targets) == 0:
        parser.error("No targets specified to clean.\nThis is never a good idea and gwf refuses to clean default targets.\n")

    if args.dry_run:
        for target in args.targets:
            print 'Cleaning', target, 'will delete the following files:'
            print workflow.targets[target].get_existing_outfiles()
            print
    else:
        for target in args.targets:
            workflow.targets[target].clean_target()
        
    sys.exit(0)

if len(args.targets) > 0:
    all_targets = args.targets
else:
    # take all terminal nodes as default targets
    all_targets = [n.target.name for n in workflow.targets.values() if len(n.dependents) == 0]


def split_tasks(tasks):
    up_to_date, in_queue, to_schedule = [], [], []
    for task in tasks:
        if task.job_in_queue:
            in_queue.append(task)
        elif task.should_run:
            to_schedule.append(task)
        else:
            up_to_date.append(task)
    return up_to_date, in_queue, to_schedule


if args.status:
    all_scheduled = set()
    for target in all_targets:

        tasks = dependencies(workflow.targets, target)
        up_to_date, in_queue, to_schedule = split_tasks(tasks)

        n_up_to_date, n_in_queue, n_to_schedule = len(up_to_date), len(in_queue), len(to_schedule)
        n_total = n_up_to_date + n_in_queue + n_to_schedule
        progress_width = 30
        in_queue_bars = int(float(n_in_queue * progress_width) / n_total)
        to_schedule_bars = int(float(n_to_schedule * progress_width) / n_total)
        up_to_date_bars = progress_width - in_queue_bars - to_schedule_bars

        progress_bar = "{}{}{}".format(COLORS['green']+'#'*up_to_date_bars,
                                       COLORS['yellow']+'#'*in_queue_bars,
                                       COLORS['red']+'#'*to_schedule_bars+CLEAR)
        print '{target} [{status}] ({count_done}/{count_queue}/{count_submit})'.format(
            target=(COLORS['bold']+target[:25]+CLEAR+'.'*35)[:35],
            status=progress_bar,
            count_done=COLORS['green']+'{}'.format(n_up_to_date).rjust(3)+CLEAR,
            count_queue=COLORS['yellow']+'{}'.format(n_in_queue).rjust(3)+CLEAR,
            count_submit=COLORS['red']+'{}'.format(n_to_schedule).rjust(3)+CLEAR)

        if args.verbose:
            for task in up_to_date:
                print COLORS['green'], task.target.name, CLEAR
            for task in in_queue:
                print COLORS['yellow'], task.target.name, CLEAR, '({}: {})'.format(task.job_id, task.job_queue_status)
            for task in to_schedule:
                print COLORS['red'], task.target.name, CLEAR
            print



else:
    # Executing work flow!
    for target_name in all_targets:
        schedule, scheduled_jobs = workflow.get_execution_schedule(target_name)

        if len(schedule) == 0:
            continue

        print '{}Scheduling computation of target'.format(COLORS['bold']), target_name, '{}...'.format(CLEAR)
        for job in schedule:

            if job.job_id:
                print "Taget", job.target.name, "is already submitted ({})".format(job.job_id)
                continue

            dependents = [dependent for dependent in job.depends_on if dependent.target.name in scheduled_jobs]

            print "Submitting target", job.target.name,
            if len(dependents) > 0 and args.verbose:
                dependents_text = ["{}[{}]".format(dependent.target.name, dependent.job_id) for dependent in dependents]
                print "(depending on {})".format(', '.join(dependents_text)),
            print '...',

            if args.dry_run:
                # For a dry run we just finish here, but write the script so people can check it
                job.write_script()
                print
                continue

            job_id = job.submit(dependents)
            print job_id
        print
